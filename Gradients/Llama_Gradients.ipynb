{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35ff6a5-3648-41b9-ae90-ed4dc70df5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab580da-a1c0-4f62-b826-673506ffa980",
   "metadata": {},
   "source": [
    "# 1. ReLU Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e76a26-c991-479e-9a33-0dfd4c845558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluGrad(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReluGrad, self).__init__()\n",
    "\n",
    "    def forward(self, dY, X):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the ReLU function.\n",
    "\n",
    "        Args:\n",
    "            dY (Tensor): Gradient with respect to the output of ReLU.\n",
    "            X (Tensor): Input tensor to ReLU.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input of ReLU.\n",
    "        \"\"\"\n",
    "        # Ensure dY and X are on the same device and dtype\n",
    "        #dY = dY.to(X.device)\n",
    "        \n",
    "        # Compute dX using the ReLU gradient formula\n",
    "        dX = torch.where(X > 0, dY, torch.zeros_like(dY))\n",
    "        \n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94306799-c380-49c6-bb70-6fad9b78a341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dY: tensor([[-1.5239, -0.5434, -1.0209],\n",
      "        [-0.8136, -1.0139,  0.9791]])\n",
      "X: tensor([[-0.5237,  2.0639, -0.1120],\n",
      "        [-0.1539,  0.6541, -0.0548]])\n",
      "dX: tensor([[ 0.0000, -0.5434,  0.0000],\n",
      "        [ 0.0000, -1.0139,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "dY = torch.randn(2, 3)  # Gradient w.r.t. ReLU output\n",
    "X = torch.randn(2, 3)   # Input tensor to ReLU\n",
    "\n",
    "# Create ReluGrad object\n",
    "relu_grad = ReluGrad()\n",
    "dX = relu_grad(dY, X)\n",
    "\n",
    "print(\"dY:\",dY)\n",
    "print(\"X:\",X)\n",
    "print(\"dX:\",dX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7971750a-403c-4b09-abfe-82931d13134a",
   "metadata": {},
   "source": [
    "# 2. QuickGeLU Grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2600328f-5d0f-468a-a22a-db96fe57ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(1.702*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c0901-0818-4385-af28-20e109a7f45d",
   "metadata": {},
   "source": [
    "### Torch gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4475e25-d519-49ae-9546-f71930e96a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0020, -0.0450,  0.5000,  1.0450,  1.0020])\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(-5, 5, 5, requires_grad=True) # requires_grad=True\n",
    "act = QuickGELU()\n",
    "out = act(x)\n",
    "out.backward(torch.ones_like(x)) # out\n",
    "x_grad = x.grad \n",
    "print(np.round(x_grad,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674a3ae-420f-46cf-ba95-43fa584fc6bc",
   "metadata": {},
   "source": [
    "### Manual gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28b53b72-c9d5-4e6c-adac-80ea2fcac310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of QuickGELU function:\n",
      " [-0.002 -0.045  0.5    1.045  1.002]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def quickgelu(x):\n",
    "    \"\"\"Compute the QuickGELU activation function.\"\"\"\n",
    "    return x * sigmoid(1.702*x)\n",
    "\n",
    "def quickgelu_grad(x):\n",
    "    \"\"\"Compute the gradient of the QuickGELU function.\"\"\"\n",
    "    sig = sigmoid(1.702*x)\n",
    "    grad = sig + x * sig * (1 - sig)*1.702\n",
    "    return grad\n",
    "\n",
    "# Example usage:\n",
    "x = np.linspace(-5, 5, 5)\n",
    "grad = quickgelu_grad(x)\n",
    "print(\"Gradient of QuickGELU function:\\n\", np.round(grad,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6f759-22ad-4430-a92c-a201e721af0a",
   "metadata": {},
   "source": [
    "# 3. Softmax Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50e1740-0acf-4371-bf78-161c59f32ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(nn.Module):\n",
    "    def forward(self, x):\n",
    "        shiftx = x -torch.max(x)\n",
    "        exps = torch.exp(shiftx)\n",
    "        return exps/torch.sum(exps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd94d0-bd6d-44e5-8d4d-f8b84edbfeb6",
   "metadata": {},
   "source": [
    "### Torch gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a69f5c09-ee88-4cbd-b800-df2a6a432db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "  return z.exp() / z.exp().sum(axis=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe7490e9-9466-4573-8061-c7a21a85b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor([[4., 2.]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76066958-bb84-4292-a874-d88786c4f98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1050, -0.1050],\n",
      "        [-0.1050,  0.1050]])\n"
     ]
    }
   ],
   "source": [
    "torch_sm = F.softmax(z, dim=1)\n",
    "\n",
    "# to extract the first row in the jacobian matrix, use [[1., 0]] \n",
    "# retain_graph=True because we re-use backward() for the second row\n",
    "torch_sm.backward(torch.tensor([[1.,0.]]), retain_graph=True) \n",
    "r1 = z.grad\n",
    "z.grad = torch.zeros_like(z) \n",
    "\n",
    "# to extract the second row in the jacobian matrix, use [[0., 1.]] \n",
    "torch_sm.backward(torch.tensor([[0.,1.]])) \n",
    "r2 = z.grad\n",
    "torch_sm_p = torch.cat((r1,r2))\n",
    "\n",
    "print(torch_sm_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3d750-3651-4421-b1ef-0283ede0a146",
   "metadata": {},
   "source": [
    "### Manual gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de92061-4fe7-47af-9d9f-601cf2b0cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_grad(z):\n",
    "  sm = softmax(z).squeeze()\n",
    "  sm_size = sm.shape[0]\n",
    "  sm_ps = []\n",
    "  for i, sm_i in enumerate(sm):\n",
    "    for j, sm_j in enumerate(sm):\n",
    "      # First case: i and j are equal:\n",
    "      if(i==j):\n",
    "        # Differentiating the softmax of a neuron w.r.t to itself\n",
    "        sm_p = sm_i * (1 - sm_i)\n",
    "        sm_ps.append(sm_p)\n",
    "      # Second case: i and j are not equal:\n",
    "      else:\n",
    "        # Differentiating the softmax of a neuron w.r.t to another neuron\n",
    "        sm_p = -sm_i * sm_j\n",
    "        sm_ps.append(sm_p)\n",
    "  sm_ps = torch.tensor(sm_ps).view(sm_size, sm_size)\n",
    "  return sm_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "539770a6-3173-441f-8792-55b85b4a2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1050, -0.1050],\n",
      "        [-0.1050,  0.1050]])\n"
     ]
    }
   ],
   "source": [
    "sm_p = softmax_grad(z)\n",
    "print(sm_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd71997-6ca2-4c08-b500-9841014cceae",
   "metadata": {},
   "source": [
    "# 4. RMS Norm (Simplified Layer Norm) Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa273df0-e6be-4634-b3b0-b5950514ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn([2, 2, 3], dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "104c86b4-2869-4897-8a1c-ce5d26094475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm(x):\n",
    "    eps = 1e-6\n",
    "    mean_sq = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "    norm_factor = torch.rsqrt(mean_sq + eps)\n",
    "    \n",
    "    # Normalize the input\n",
    "    y = x * norm_factor\n",
    "    return y, norm_factor, mean_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e13a16-c72f-4263-ba44-e5ba1567dc1f",
   "metadata": {},
   "source": [
    "### Torch gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79676045-62e1-4244-96ef-1e3b590ec638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7736, 0.6207, 0.4635],\n",
      "         [1.2253, 0.3718, 0.2097]],\n",
      "\n",
      "        [[1.2487, 1.0069, 1.8734],\n",
      "         [0.9376, 0.2695, 2.9609]]])\n"
     ]
    }
   ],
   "source": [
    "y, norm_factor, mean_sq = rms_norm(x)\n",
    "y.sum().backward()  # Backward pass to compute gradients\n",
    "autograd_grad = x.grad\n",
    "print(autograd_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2160b-b847-4f18-83cb-d440eccf4a55",
   "metadata": {},
   "source": [
    "### Manual gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91b480b0-6a25-4cdc-8205-0b789f45e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm_grad(x):\n",
    "    # Forward pass\n",
    "    y, norm_factor, mean_sq = rms_norm(x)\n",
    "    \n",
    "    \n",
    "    # Compute gradients manually\n",
    "    # Gradient of mean_sq with respect to x\n",
    "    grad_mean_sq = 2 * x.mean(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Gradient of norm_factor with respect to mean_sq\n",
    "    grad_norm_factor = -0.5 * (mean_sq + 1e-6).pow(-1.5) * grad_mean_sq\n",
    "    \n",
    "    # Gradient of y = x * norm_factor\n",
    "    grad_y = norm_factor + x * grad_norm_factor\n",
    "    \n",
    "    return grad_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2457e937-1701-46be-8798-7b56472e1d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7736, 0.6207, 0.4635],\n",
      "         [1.2253, 0.3718, 0.2097]],\n",
      "\n",
      "        [[1.2487, 1.0069, 1.8734],\n",
      "         [0.9376, 0.2695, 2.9609]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "manual_grad = rms_norm_grad(x)\n",
    "print(manual_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2b4e8-c508-45e9-8a47-52039604e5e7",
   "metadata": {},
   "source": [
    "# 5. Layer Norm Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54e068e8-7165-4486-b898-066defecb72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "d = 4\n",
    "x = torch.randn(2,3,d,requires_grad=True)\n",
    "dout = torch.rand(2,3,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c02ad65-e1f7-42ca-9373-dca2560951e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = nn.LayerNorm(d)\n",
    "y = layer_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643a98e-c4b2-427f-a8b2-c8d0d2bd4002",
   "metadata": {},
   "source": [
    "### Torch gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2792cb2-2ed9-4851-acf8-51d320ee09cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " x.grad: tensor([[[ 0.4349, -0.2657,  0.6004, -0.7697],\n",
      "         [-0.1073, -0.0484,  0.2523, -0.0966],\n",
      "         [-0.6142, -0.1115,  0.0653,  0.6604]],\n",
      "\n",
      "        [[ 0.2990, -0.0633,  0.6517, -0.8875],\n",
      "         [ 0.4394, -0.2104, -0.0678, -0.1612],\n",
      "         [ 0.1001,  0.1124, -0.3442,  0.1317]]])\n"
     ]
    }
   ],
   "source": [
    "fakeloss=(y*dout).sum()\n",
    "fakeloss.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4a6f7-385f-4d1c-b48b-645b3f58407f",
   "metadata": {},
   "source": [
    "### Manual gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c26fb480-97bf-4cb2-8bed-f25ca6213e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=1e-10\n",
    "\n",
    "class LayerNorm:\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x, w, b):\n",
    "        # x is the input activations, of shape B,T,C\n",
    "        # w are the weights, of shape C\n",
    "        # b are the biases, of shape C\n",
    "        B, T, C = x.size()\n",
    "        # calculate the mean\n",
    "        mean = x.sum(-1, keepdim=True) / C # B,T,1\n",
    "        # calculate the variance\n",
    "        xshift = x - mean # B,T,C\n",
    "        var = (xshift**2).sum(-1, keepdim=True) / C # B,T,1\n",
    "        # calculate the inverse standard deviation: **0.5 is sqrt, **-0.5 is 1/sqrt\n",
    "        rstd = (var + eps) ** -0.5 # B,T,1\n",
    "        # normalize the input activations\n",
    "        norm = xshift * rstd # B,T,C\n",
    "        # scale and shift the normalized activations at the end\n",
    "        out = norm * w + b # B,T,C\n",
    "\n",
    "        # return the output and the cache, of variables needed later during the backward pass\n",
    "        cache = (x, w, mean, rstd)\n",
    "        return out, cache\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(dout, cache):\n",
    "        x, w, mean, rstd = cache\n",
    "        # recompute the norm (save memory at the cost of compute)\n",
    "        norm = (x - mean) * rstd\n",
    "        # gradients for weights, bias\n",
    "        db = dout.sum((0, 1))\n",
    "        dw = (dout * norm).sum((0, 1))\n",
    "        # gradients for input\n",
    "        dnorm = dout * w\n",
    "        dx = dnorm - dnorm.mean(-1, keepdim=True) - norm * (dnorm * norm).mean(-1, keepdim=True)\n",
    "        dx *= rstd\n",
    "        return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3b46365-83c1-4999-8bc5-b79bd4943fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "B = 2 # some toy numbers here\n",
    "T = 3\n",
    "C = 4\n",
    "x = torch.randn(B, T, C, requires_grad=True)\n",
    "w = torch.ones(4,requires_grad=True)\n",
    "b = torch.zeros(4, requires_grad=True)\n",
    "dout = torch.rand(B,T,C)\n",
    "out, cache = LayerNorm.forward(x,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b231f56-1f49-474d-b4df-3b06f59d832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4349, -0.2657,  0.6004, -0.7697],\n",
      "         [-0.1073, -0.0484,  0.2523, -0.0966],\n",
      "         [-0.6142, -0.1115,  0.0653,  0.6604]],\n",
      "\n",
      "        [[ 0.2990, -0.0633,  0.6518, -0.8875],\n",
      "         [ 0.4394, -0.2104, -0.0678, -0.1612],\n",
      "         [ 0.1001,  0.1124, -0.3442,  0.1317]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dx, dw, db = LayerNorm.backward(dout, cache)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b46a5-d9ee-4a0d-b827-527e9515af7d",
   "metadata": {},
   "source": [
    "# 6. SoftmaxCrossEntropyLoss Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac21e02e-c9ef-46f2-8d6b-2279d37dbbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logits and class indices\n",
    "logits = torch.tensor([\n",
    "    [2.0, 1.0, 0.1, 0.5, 1.5],  # Sample 1\n",
    "    [0.2, 2.1, 0.4, 1.0, -0.5],  # Sample 2\n",
    "    [1.0, 0.0, 2.5, 0.5, 1.2]    # Sample 3\n",
    "], requires_grad=True)  # Logits with requires_grad=True\n",
    "\n",
    "class_indices = torch.tensor([2, 0, 3])  # True class indices for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25303c93-6362-4fcb-abe2-179a76fc66c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the softmax probabilities\n",
    "def softmax(x):\n",
    "    exps = torch.exp(x - torch.max(x, dim=1, keepdim=True).values)\n",
    "    return exps / exps.sum(dim=1, keepdim=True)\n",
    "\n",
    "# Compute softmax probabilities\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "# Compute the cross-entropy loss manually\n",
    "def cross_entropy_loss(probabilities, class_indices):\n",
    "    log_probs = torch.log(probabilities)\n",
    "    loss = -log_probs[range(logits.size(0)), class_indices].mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d215970-7f6f-4ce0-a1a7-ecc3950c05bf",
   "metadata": {},
   "source": [
    "### Torch gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57170377-2ac5-4979-9955-087308252241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1420,  0.0522, -0.3121,  0.0317,  0.0861],\n",
      "        [-0.3047,  0.1916,  0.0350,  0.0638,  0.0142],\n",
      "        [ 0.0434,  0.0160,  0.1946, -0.3070,  0.0530]])\n"
     ]
    }
   ],
   "source": [
    "loss = cross_entropy_loss(probabilities, class_indices)\n",
    "loss.backward()\n",
    "print(logits.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eea4e5-0d7c-488d-ac4d-b1acab78b9a5",
   "metadata": {},
   "source": [
    "### Manual gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afa24fcc-d994-40d9-9f30-b750f9f31711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(logits, probabilities, class_indices):\n",
    "    # One-hot encode the true class indices\n",
    "    one_hot_labels = torch.zeros_like(probabilities)\n",
    "    one_hot_labels[range(logits.size(0)), class_indices] = 1\n",
    "    \n",
    "    # Gradient of the loss with respect to logits\n",
    "    grad_logits = (probabilities - one_hot_labels) /logits.size(0)\n",
    "    return grad_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09c197bd-278e-4470-a552-37ffc738063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1420,  0.0522, -0.3121,  0.0317,  0.0861],\n",
      "        [-0.3047,  0.1916,  0.0350,  0.0638,  0.0142],\n",
      "        [ 0.0434,  0.0160,  0.1946, -0.3070,  0.0530]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "manual_grad_logits = compute_gradients(logits, probabilities, class_indices)\n",
    "print(manual_grad_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa61ddc-8d9b-4f93-a15a-9661d84eb4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
